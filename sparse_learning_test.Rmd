---
title: "sparse_learning_test"
author: "Linxuan Wang"
date: "2023-10-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(qrjoint)
library(tidyverse)
library(gridExtra)
source("coefL.R")
source("qrjointL.R")
source("estFnL.R")
source("summaryL.R")
source("predL.R")
source("ppFnL.R")
```

```{r,beta.0 and beta, sparse setting}
p = 8 # # of predictors
b0_05 <- 0
b_05 <- matrix(c(0.96,-0.38,0.05,-0.22,-0.80,-0.80,-5.97,2.50),p,1)
a <- matrix(c(0,-3,0, 0,0,-2, -3,0,2, -2,2,2, 0,4,-4, 5,1,0, -1,0,0, 1,-3,2),3,p)
b0.dot <- function(tau){
  return (1/(tau*(1-tau)))
}

b.dot <- function(tau){
  v = t(a)%*%matrix(c(dnorm(tau,0,1/3),dnorm(tau,1/2,1/3),dnorm(tau,1,1/3)),3,1)
  # impose sparsity
  v[5] = 0
  if (tau < 0.5){
    v[8] = 0
  }
  return(b0.dot(tau)*v/(sqrt(1+norm(v,"2")^2)))
}
```

```{r,beta.0 and beta, paper setting}
p = 7 # # of predictors
b0_05 <- 0
b_05 <- matrix(c(0.96,-0.38,0.05,-0.22,-0.80,-0.80,-5.97),p,1)
a <- matrix(c(0,-3,0, 0,0,-2, -3,0,2, -2,2,2, 0,4,-4, 5,1,0, -1,0,0),3,p)
b0.dot <- function(tau){
  return (1/(tau*(1-tau)))
}

b.dot <- function(tau){
  v = t(a)%*%matrix(c(dnorm(tau,0,1/3),dnorm(tau,1/2,1/3),dnorm(tau,1,1/3)),3,1)
  
  return(b0.dot(tau)*v/(sqrt(1+norm(v,"2")^2)))
}
```

```{r, coordinate function}
bj <- function(tau,j) { sapply(tau, function(t) b.dot(t)[j]) }
```

```{r, visualize derivatives}
taug <- seq(0.01,0.99,0.01)
betadot.true <- matrix(NA,99,p + 1)
for (i in 1:99){
  b0 <- b0.dot(taug[i])
  b <- sapply(seq(1,p),function (j) {bj(taug[i],j)})
  betadot.true[i,] <- c(b0,b)
}
betadot.true <- cbind(data.frame(betadot.true),taug)
# Generate plots for each column using the last column as x-axis
df <- as.data.frame(betadot.true)

plots <- lapply(1:(ncol(df)-1), function(i) {
  ggplot(df, aes_string(x = names(df)[ncol(df)], y = names(df)[i])) +
    geom_line() +
    labs(title = paste("Plot of", names(df)[i])) +
    theme_minimal()
})

# Arrange the plots together
do.call(grid.arrange, plots)
```


```{r, true beta}
taug <- seq(0.01,0.99,0.01)
beta.true <- matrix(NA,99,p + 1)
for (i in 1:99){
  b0 <- b0_05 + integrate(b0.dot,0.5,taug[i])$value
  b <- sapply(seq(1,p),function (j) {b_05[j] + integrate(function (tau){bj(tau,j)},0.5,taug[i])$value})
  beta.true[i,] <- c(b0,b)
}
beta.true.tau <- cbind(data.frame(beta.true),taug)
```

```{r, visualize true betas}
df = as.data.frame(beta.true.tau)

# Generate plots for each column using the last column as x-axis
plots <- lapply(1:(ncol(df)-1), function(i) {
  ggplot(df, aes_string(x = names(df)[ncol(df)], y = names(df)[i])) +
    geom_line() +
    labs(title = paste("Plot of", names(df)[i])) +
    theme_minimal()
})

# Arrange the plots together
do.call(grid.arrange, plots)
```



```{r, DGP}
set.seed(1026)
n <- 1000
Z <- matrix(rnorm(n*p,0,1),n,p)
Z.norm <- apply(Z, 1, function(row) norm(row, type="2"))
U1 <- rbeta(n,8,1)
X <- U1*Z/Z.norm

q_y <- function(x,tau){
  b0 <- b0_05 + integrate(b0.dot,0.5,tau)$value
  b <- sapply(seq(1,p),function (j) {b_05[j] + integrate(function (tau){bj(tau,j)},0.5,tau)$value})
  
  q <- b0 + x%*%matrix(b,p,1)
  return(q)
}

U2 <- runif(n,0,1)

Y <- mapply(FUN = function(row_idx, u2_element) {
  q_y(X[row_idx, , drop = FALSE], u2_element)
}, row_idx = 1:nrow(X), u2_element = U2, SIMPLIFY = FALSE)
Y <- do.call(rbind, Y)

sim_data <- data.frame(cbind(X,Y))
```

### ESTIMATION UNDER SPARSE LEARNING SETTING ###

```{r}
nsamp = 100
thin = 20
cens = NULL
wt = NULL
incr = 0.01
par = "prior"
nknots = 6
hyper = list(sig = c(0.1, 0.1), lam = c(6, 4), kap = c(0.1, 0.1, 1))
shrink = FALSE
prox.range = c(0.2, 0.95)# range of lambda_j in the paper (range on rho_0.1(lambda))
acpt.target = 0.15# target acceptance rate of adaptive Metropolis sampler
ref.size = 3 # the proposed density is updated once every ref.size iterations
blocking = "std5" # same as in the paper
temp = 1 # log-likelihood is raised to this param
expo = 2 # 2 correspond to square-exponential parameter
#blocks.mu, blocks.S, 
fix.nu = FALSE
fbase = c("t", "logistic", "unif")
verbose = TRUE
sparse = TRUE
```

```{r}
#function (formula, data, nsamp = 100, thin = 10, cens = NULL, 
#    wt = NULL, incr = 0.01, par = "prior", nknots = 6, hyper = list(sig = c(0.1, 
#        0.1), lam = c(6, 4), kap = c(0.1, 0.1, 1)), shrink = FALSE, 
#    prox.range = c(0.2, 0.95), # range of lambda_j in the paper (range on rho_0.1(lambda))
#    acpt.target = 0.15, # target acceptance rate of adaptive Metropolis sampler
#    ref.size = 3, # the proposed density is updated once every ref.size iterations
#    blocking = "std5", # same as in the paper
#    temp = 1, # log-likelihood is raised to this param
#    expo = 2, # 2 correspond to square-exponential parameter
#    blocks.mu, blocks.S, 
#    fix.nu = FALSE, fbase = c("t", "logistic", "unif"), verbose = TRUE, sparse = TRUE) # additional parameter for sparse learning
#{
    fbase.choice <- match(fbase[1], c("t", "logistic", "unif"))
    if (is.na(fbase.choice)) 
        stop("Only 't', 'logistic' or 'unif' is allowed for the choice of fbase")
    if (fbase.choice == 1) { # t-distribution is used as the base
        q0 <- function(u, nu = Inf) return(1/(dt(qt(qrjoint:::unitFn(u), # unitFn is an internal function, the usage of which allows ``probabilities`` greater than 1, but why?
            df = nu), df = nu) * qt(0.9, df = nu))) 
        Q0 <- function(u, nu = Inf) return(qt(qrjoint:::unitFn(u), df = nu)/qt(0.9, 
            df = nu))
        F0 <- function(x, nu = Inf) return(pt(x * qt(0.9, df = nu), 
            df = nu))
    } else if (fbase.choice == 2) { # logistic base
        fix.nu <- 1 # nu is fixed and excluded from the MCMC updates
        q0 <- function(u, nu = Inf) return(1/dlogis(qlogis(unitFn(u))))
        Q0 <- function(u, nu = Inf) return(qlogis(unitFn(u)))
        F0 <- function(x, nu = Inf) return(plogis(x))
    } else {
        fix.nu <- 1
        q0 <- function(u, nu = Inf) return(1/(dunif(qunif(u, 
            -1, 1), -1, 1)))
        Q0 <- function(u, nu = Inf) return(qunif(u, -1, 1))
        F0 <- function(x, nu = Inf) return(punif(x, -1, 1))
    }
    base.bundle <- list(q0 = q0, Q0 = Q0, F0 = F0)
    # to debug the code below, run the code below
    mf = match.call(qrjoint, call("qrjoint", X9 ~ ., sim_data, nsamp = 500, thin = 10), expand.dots = F)

    #mf <- match.call(expand.dots = FALSE)
    m <- match(c("formula", "data", "cens", "wt"), names(mf), 
        0L)
    mf <- mf[c(1L, m)]
    mf$drop.unused.levels <- TRUE
    mf[[1L]] <- quote(stats::model.frame)
    mf <- eval(mf, parent.frame())
    mt <- attr(mf, "terms")
    if (!attr(mt, "intercept")) 
        stop("Model formula must contain an intercept. See 'qde' formula for density estimation.")
    if (sum(attr(mt, "order")) == 0) 
        stop("Model formula must contain at least one non-intercept term. See 'qde' formula for density estimation.")
    y <- model.response(mf, "numeric") # value of response variables
    if (length(dim(y)) > 1) 
        stop("Model response must be univariate.")
    x <- model.matrix(mt, mf)[, -1, drop = FALSE]
    n <- nrow(x)
    p <- ncol(x)
    x <- scale(x, qrjoint:::chull.center(x))
    illcond <- (qr(x)$rank < p)
    cens <- model.extract(mf, "cens")
    wt <- model.extract(mf, "wt")
    if (illcond & par != "noX") {
        warning("Ill condiitoned design matrix. Changing parameter initialization to 'noX'")
        par <- "noX"
    }
    if (is.null(cens)) {
      cens <- rep(0, n)
    } else {
        cens <- as.vector(cens)
        if (!is.numeric(cens)) 
            stop("'cens' must be a numeric vector")
        if (length(cens) != n) 
            stop(gettextf("number of cens is %d, should equal %d (number of observations)", 
                length(cens), n), domain = NA)
    }
    
    if (is.null(wt)) {
      wt <- rep(1, n)
    } else {
        wt <- as.vector(wt)
        if (!is.numeric(wt)) 
            stop("'wt' must be a numeric vector")
        if (length(wt) != n) 
            stop(gettextf("number of wt is %d, should equal %d (number of observations)", 
                length(wt), n), domain = NA)
    }
    # incr = 0.01
    
    # below implements a data-dependent grid
    Ltail <- ceiling(2 * log(n, 2) + log(incr, 2))
    if (Ltail > 0) { # compare n^2*incr and 1 to see whether sample size is big enough to estimate a finer grid.
        tau.tail <- incr/2^(Ltail:1) # until we recovered 1-1/n^2 (differs from 1-1/2n as in the paper)
        tau.g <- c(0, tau.tail, seq(incr, 1 - incr, incr), 1 - 
            tau.tail[Ltail:1], 1) # the grid becomes finer at quantiles between 0 and 0.01 as well as between 0.99 and 1.
        L <- length(tau.g)
        mid <- which.min(abs(tau.g - 0.5))
        reg.ix <- (1:L)[-c(1 + 1:Ltail, L - 1:Ltail)]
    } else { # sample size isn't big enough
        tau.g <- seq(0, 1, incr)
        L <- length(tau.g)
        mid <- which.min(abs(tau.g - 0.5))
        reg.ix <- (1:L)
    }
    nknots = 6
    tau.kb <- seq(0, 1, len = nknots) # knots for interpolation approximation for w_j's
    tau.k <- tau.kb
    # hyper = list(sig = c(0.1, 0.1), lam = c(6, 4), kap = c(0.1, 0.1, 1))
    a.sig <- hyper$sig
    if (is.null(a.sig)) 
        a.sig <- c(0.1, 0.1)
    a.lam <- hyper$lam
    if (is.null(a.lam)) 
        a.lam <- c(6, 4)
    a.kap <- hyper$kap
    if (is.null(a.kap)) 
        a.kap <- c(0.1, 0.1, 1)
    a.kap <- matrix(a.kap, nrow = 3)
    nkap <- ncol(a.kap) # number of gamma mixtures in the prior distribution of kappa
    a.kap[3, ] <- log(a.kap[3, ])
    hyper.reduced <- c(a.sig, c(a.kap)) # no parameter for lambda
    # prox.range = c(0.2, 0.95)
    # selecting prior for lambda to guarantee priors remain sufficiently overlapped for neighboring lambda_g values
    prox.grid <- qrjoint:::proxFn(max(prox.range), min(prox.range), 0.5) 
    ngrid <- length(prox.grid)
    lamsq.grid <- qrjoint:::lamFn(prox.grid)^2 # grid for lambda^2
    prior.grid <- -diff(pbeta(c(1, (prox.grid[-1] + prox.grid[-ngrid])/2, 
        0), a.lam[1], a.lam[2])) # diff finalizes the discretization
    lp.grid <- log(prior.grid) # log probability
    # expo = 2
    d.kg <- abs(outer(tau.k, tau.g, "-"))^expo # tau.g is the data-dependent grid on which we need to keep track of w_j's
    d.kk <- abs(outer(tau.k, tau.k, "-"))^expo # tau.k is the uniformly spaced knots grid to decrease complexity
    gridmats <- matrix(NA, nrow = nknots * (L + nknots) + 2, ncol = ngrid)
    K0 <- 0
    t1 <- Sys.time()
    for (i in 1:ngrid) { # for each lambda squared
        K.grid <- exp(-lamsq.grid[i] * d.kg) # calculate kernel matrix C_{0*}(sqrt(lamsq.grid[i])
        K.knot <- exp(-lamsq.grid[i] * d.kk) # calculate kernel matrix C_{**}(sqrt(lamsq.grid[i])
        diag(K.knot) <- 1 + 1e-10
        R.knot <- chol(K.knot)
        A.knot <- solve(K.knot, K.grid) # A_g on page 1112
        gridmats[, i] <- c(c(A.knot), c(R.knot), sum(log(diag(R.knot))), # calculates sum of the logarithm of the eigenvalues. 
            lp.grid[i])
        K0 <- K0 + prior.grid[i] * K.knot # times the prior probability of lambda_g with the kernel matrix
    }
    t2 <- Sys.time()
    # nsamp <- 1000
    # thin <- 10
    niter <- nsamp * thin
    dimpars <- c(n, p, L, mid - 1, nknots, ngrid, ncol(a.kap), # L: number of tau's on which we keep track of w_j
        niter, thin, nsamp)
    if (par[1] == "prior") {
        par <- rep(0, (nknots + 1) * (p + 1) + 2) # initialize parameters
        # fix.nu = FALSE
        if (fix.nu) 
            par[(nknots + 1) * (p + 1) + 2] <- qrjoint:::nuFn.inv(fix.nu)
        # library(quantreg)
        beta.rq <- sapply(tau.g, function(a) return(coef(suppressWarnings(quantreg::rq(quantreg::dither(y) ~ 
            x, tau = a, weights = wt))))) # perform quantile regression at a single quantile for 121 quantiles
        # library(splines)
        v <- splines::bs(tau.g, df = 5) # two knots for cubic splines
        rq.lm <- apply(beta.rq, 1, function(z) return(coef(lm(z ~ 
            v)))) # regress each beta_j on splines generated by quantiles (why do we do that? seems like we want to use quantiles to explain beta's)
        delta <- tau.g[2] # first nonzero quantile
        tau.0 <- tau.g[mid] # middle quantile
        rq.tau0 <- c(c(1, predict(v, tau.0)) %*% rq.lm) # predict(v,tau.0) returns row 61 in v, this line returns "middle beta"
        # rq.delta and rq.deltac are not used in this case
        rq.delta <- c(c(1, predict(v, delta)) %*% rq.lm) # predict(v,delta) returns row 2 in v
        rq.deltac <- c(c(1, predict(v, 1 - delta)) %*% rq.lm) # predict(v,1-delta) returns row 120 in v
        par[nknots * (p + 1) + 1:(p + 1)] <- as.numeric(rq.tau0) # initialize gamma and gamma_0
        sigma <- 1
        par[(nknots + 1) * (p + 1) + 1] <- qrjoint:::sigFn.inv(sigma, a.sig) # initialize sigma
        for (i in 1:(p + 1)) {
            kapsq <- sum(exp(a.kap[3, ]) * (a.kap[2, ]/a.kap[1, 
                ])) # initialize kappa squared as the prior mean of mixture of gamma distributions
            lam.ix <- sample(length(lamsq.grid), 1, prob = prior.grid) # for each w_j, randomly select a lambda squared out of the grid according to prior.grid (discretized distribution)
            R <- matrix(gridmats[L * nknots + 1:(nknots * nknots), 
                lam.ix], nknots, nknots) # cholesky decomp of kernel matrix C_{**}(sqrt(lamsq.grid[lam.ix])
            z <- sqrt(kapsq) * c(crossprod(R, rnorm(nknots))) #t(R)%*%rnorm(nknots), this line randomly draws a sample from the multivariate normal N(0,k_j^2*c^{SE}(.,.|lambda_j))
            par[(i - 1) * nknots + 1:nknots] <- z - mean(z)
        }
        beta.hat <- qrjoint:::estFn(par, x, y, gridmats, L, mid, nknots, 
            ngrid, a.kap, a.sig, tau.g, reg.ix, FALSE, base.bundle = base.bundle) # generate beta0 and beta
        qhat <- tcrossprod(cbind(1, x), beta.hat)
        infl <- max(max((y - qhat[, mid])/(qhat[, ncol(qhat) - 
            1] - qhat[, mid])), max((qhat[, mid] - y)/(qhat[, 
            mid] - qhat[, 2])))
        # shrink = FALSE
        oo <- .C("INIT", par = as.double(par), x = as.double(x), 
            y = as.double(y), cens = as.integer(cens), wt = as.double(wt), 
            shrink = as.integer(shrink), hyper = as.double(hyper.reduced), 
            dim = as.integer(dimpars), gridpars = as.double(gridmats), 
            tau.g = as.double(tau.g), siglim = as.double(qrjoint:::sigFn.inv(c(1 * 
                infl * sigma, 10 * infl * sigma), a.sig)), fbase.choice = as.integer(fbase.choice))
        par <- oo$par
    } else if (par[1] == "RQ") {
        par <- rep(0, (nknots + 1) * (p + 1) + 2)
        beta.rq <- sapply(tau.g, function(a) return(coef(suppressWarnings(quantreg::rq(quantreg::dither(y) ~ 
            x, tau = a, weights = wt)))))
        v <- bs(tau.g, df = 5)
        rq.lm <- apply(beta.rq, 1, function(z) return(coef(lm(z ~ 
            v))))
        delta <- tau.g[2]
        tau.0 <- tau.g[mid]
        rq.tau0 <- c(c(1, predict(v, tau.0)) %*% rq.lm)
        rq.delta <- c(c(1, predict(v, delta)) %*% rq.lm)
        rq.deltac <- c(c(1, predict(v, 1 - delta)) %*% rq.lm)
        par[nknots * (p + 1) + 1:(p + 1)] <- as.numeric(rq.tau0)
        nu <- ifelse(fix.nu, fix.nu, nuFn(0))
        sigma <- min((rq.delta[1] - rq.tau0[1])/Q0(delta, nu), 
            (rq.deltac[1] - rq.tau0[1])/Q0(1 - delta, nu))
        par[(nknots + 1) * (p + 1) + 1] <- sigFn.inv(sigma, a.sig)
        epsilon <- 0.1 * min(diff(sort(tau.k)))
        tau.knot.plus <- pmin(tau.k + epsilon, 1)
        tau.knot.minus <- pmax(tau.k - epsilon, 0)
        beta.rq.plus <- cbind(1, predict(v, tau.knot.plus)) %*% 
            rq.lm
        beta.rq.minus <- cbind(1, predict(v, tau.knot.minus)) %*% 
            rq.lm
        zeta0.plus <- F0((beta.rq.plus[, 1] - rq.tau0[1])/sigma, 
            nu)
        zeta0.minus <- F0((beta.rq.minus[, 1] - rq.tau0[1])/sigma, 
            nu)
        zeta0.dot.knot <- (zeta0.plus - zeta0.minus)/(tau.knot.plus - 
            tau.knot.minus)
        w0.knot <- log(pmax(epsilon, zeta0.dot.knot))/shrinkFn(p)
        w0.knot <- (w0.knot - mean(w0.knot))
        w0PP <- ppFn0(w0.knot, gridmats, L, nknots, ngrid)
        w0 <- w0PP$w
        zeta0.dot <- exp(shrinkFn(p) * (w0 - max(w0)))
        zeta0 <- trape(zeta0.dot[-c(1, L)], tau.g[-c(1, L)], 
            L - 2)
        zeta0.tot <- zeta0[L - 2]
        zeta0 <- c(0, tau.g[2] + (tau.g[L - 1] - tau.g[2]) * 
            zeta0/zeta0.tot, 1)
        zeta0.dot <- (tau.g[L - 1] - tau.g[2]) * zeta0.dot/zeta0.tot
        zeta0.dot[c(1, L)] <- 0
        par[1:nknots] <- w0.knot
        beta0.dot <- sigma * q0(zeta0, nu) * zeta0.dot
        tilt.knot <- tau.g[tilt.ix <- sapply(tau.k, function(a) which.min(abs(a - 
            zeta0)))]
        tau.knot.plus <- pmin(tilt.knot + epsilon, 1)
        tau.knot.minus <- pmax(tilt.knot - epsilon, 0)
        beta.rq.plus <- cbind(1, predict(v, tau.knot.plus)) %*% 
            rq.lm
        beta.rq.minus <- cbind(1, predict(v, tau.knot.minus)) %*% 
            rq.lm
        beta.dot.knot <- (beta.rq.plus[, -1, drop = FALSE] - 
            beta.rq.minus[, -1, drop = FALSE])/(tau.knot.plus - 
            tau.knot.minus)
        par[nknots + 1:(nknots * p)] <- c(beta.dot.knot)
        beta.hat <- estFn(par, x, y, gridmats, L, mid, nknots, 
            ngrid, a.kap, a.sig, tau.g, reg.ix, FALSE, base.bundle = base.bundle)
        qhat <- tcrossprod(cbind(1, x), beta.hat)
        infl <- max(max((y - qhat[, mid])/(qhat[, ncol(qhat) - 
            1] - qhat[, mid])), max((qhat[, mid] - y)/(qhat[, 
            mid] - qhat[, 2])))
        oo <- .C("INIT", par = as.double(par), x = as.double(x), 
            y = as.double(y), cens = as.integer(cens), wt = as.double(wt), 
            shrink = as.integer(shrink), hyper = as.double(hyper.reduced), 
            dim = as.integer(dimpars), gridpars = as.double(gridmats), 
            tau.g = as.double(tau.g), siglim = as.double(sigFn.inv(c(1 * 
                infl * sigma, 10 * infl * sigma), a.sig)), fbase.choice = as.integer(fbase.choice))
        par <- oo$par 
    } else if (par[1] == "noX") {
        fit0 <- qde(y, nsamp = 100, thin = 10, cens = cens, wt = wt, 
            nknots = nknots, hyper = hyper, prox.range = prox.range, 
            fbase = fbase, fix.nu = fix.nu, verbose = FALSE)
        par <- rep(0, (nknots + 1) * (p + 1) + 2)
        par[c(1:nknots, nknots * (p + 1) + 1, (nknots + 1) * 
            (p + 1) + 1:2)] <- fit0$par
    } else {
        if (!is.numeric(par)) 
            stop(paste0("'par' must be initialized either as a numeric vector of length ", 
                (nknots + 1) * (p + 1) + 2, " or, as one of the following strings: 'prior', 'RQ', 'noX'"))
        if (length(par) != (nknots + 1) * (p + 1) + 2) 
            stop(paste0("'par' numeric vector must have length = ", 
                (nknots + 1) * (p + 1) + 2))
    }
```

```{r}
    # include sparse learning settings
if (sparse){ # execute sparse learning
    par <- c(par,rep(1,p)) # add delta_j's initial values
    npar <- (nknots + 1) * (p + 1) + 2 + p # additional delta_j's for j = 1,...,p
    blocks <- replicate(2 * p + 3, rep(FALSE, npar), simplify = FALSE) # additional blocks for updating delta_j's
    for (i in 0:p) blocks[[i + 1]][c(i * nknots + 1:nknots, 
          nknots * (p + 1) + i + 1)] <- TRUE # update w_i and \gamma_i
    blocks[[p + 2]][nknots * (p + 1) + 1:(p + 1)] <- TRUE # update \gamma_0 and \gamma
    blocks[[p + 3]][(nknots + 1) * (p + 1) + 1:2] <- TRUE # update \sigma and \nu
    for (i in 1:p) blocks[[p + 3 + i]][(nknots + 1) * (p + 1) + 2 + i] <-  TRUE # update delta_j separately
    
    blocks.ix <- c(unlist(lapply(blocks, which))) - 1
    blocks.size <- sapply(blocks, sum) # #params to be update in each block
    # number of blocks
    nblocks <- length(blocks)
    
    #if (missing(blocks.mu)) 
        blocks.mu <- rep(0, sum(blocks.size))
    #if (missing(blocks.S)) {
        sig.nu <- c(TRUE, !as.logical(fix.nu))
        blocks.S <- lapply(blocks.size, function(q) diag(1, q))
        for (i in 1:(p + 1)) blocks.S[[i]][1:nknots, 1:nknots] <- K0 # first p+1 blocks
        if (!illcond) {
          blocks.S[[p + 2]] <- summary(suppressWarnings(quantreg::rq(quantreg::dither(y) ~ 
            x, tau = 0.5, weights = wt)), se = "boot", 
            cov = TRUE)$cov
        }
        blocks.S[[p + 3]] <- matrix(c(1, 0, 0, 0.1), 2, 2)[sig.nu, sig.nu]
        for (i in 1:p) blocks.S[[p + 3 + i]] <- 1 # initial variance of delta_j's
        blocks.S <- unlist(blocks.S)
    #}
    
  } else { # sparse learning isn't applied
      npar <- (nknots + 1) * (p + 1) + 2 
      if (blocking == "single") {
        blocks <- list(rep(TRUE, npar))
    }
      else if (blocking == "single2") {
        blocks <- list(rep(TRUE, npar), rep(FALSE, npar))
        blocks[[2]][nknots * (p + 1) + 1:(p + 3)] <- TRUE
    }
      else if (blocking == "single3") {
        blocks <- list(rep(TRUE, npar), rep(FALSE, npar), rep(FALSE, 
            npar))
        blocks[[2]][nknots * (p + 1) + 1:(p + 1)] <- TRUE
        blocks[[3]][(nknots + 1) * (p + 1) + 1:2] <- TRUE
    }
      else if (blocking == "std0") {
        blocks <- replicate(p + 1, rep(FALSE, npar), simplify = FALSE)
        for (i in 0:p) blocks[[i + 1]][c(i * nknots + 1:nknots, 
            nknots * (p + 1) + i + 1, (nknots + 1) * (p + 1) + 
                1:2)] <- TRUE
    }
      else if (blocking == "std1") {
        blocks <- replicate(p + 2, rep(FALSE, npar), simplify = FALSE)
        for (i in 0:p) blocks[[i + 1]][c(i * nknots + 1:nknots, 
            nknots * (p + 1) + i + 1, (nknots + 1) * (p + 1) + 
                1:2)] <- TRUE
        blocks[[p + 2]][nknots * (p + 1) + 1:(p + 3)] <- TRUE
    }
      else if (blocking == "std2") {
        blocks <- replicate(p + 3, rep(FALSE, npar), simplify = FALSE)
        for (i in 0:p) blocks[[i + 1]][c(i * nknots + 1:nknots, 
            nknots * (p + 1) + i + 1, (nknots + 1) * (p + 1) + 
                1:2)] <- TRUE
        blocks[[p + 2]][nknots * (p + 1) + 1:(p + 1)] <- TRUE
        blocks[[p + 3]][(nknots + 1) * (p + 1) + 1:2] <- TRUE
    }
      else if (blocking == "std3") {
        blocks <- replicate(p + 3, rep(FALSE, npar), simplify = FALSE)
        for (i in 0:p) blocks[[i + 1]][c(i * nknots + 1:nknots)] <- TRUE
        blocks[[p + 2]][nknots * (p + 1) + 1:(p + 1)] <- TRUE
        blocks[[p + 3]][(nknots + 1) * (p + 1) + 1:2] <- TRUE
    }
      else if (blocking == "std4") {
        blocks <- replicate(p + 3, rep(FALSE, npar), simplify = FALSE)
        for (i in 0:p) blocks[[i + 1]][c(i * nknots + 1:nknots, 
            nknots * (p + 1) + i + 1)] <- TRUE
        blocks[[p + 2]][nknots * (p + 1) + 1:(p + 1)] <- TRUE
        blocks[[p + 3]][(nknots + 1) * (p + 1) + 1:2] <- TRUE
    }
      else if (blocking == "std5") {
        blocks <- replicate(p + 4, rep(FALSE, npar), simplify = FALSE)
        for (i in 0:p) blocks[[i + 1]][c(i * nknots + 1:nknots, 
            nknots * (p + 1) + i + 1)] <- TRUE # update w_i and \gamma_i
        blocks[[p + 2]][nknots * (p + 1) + 1:(p + 1)] <- TRUE # update \gamma_0 and \gamma
        blocks[[p + 3]][(nknots + 1) * (p + 1) + 1:2] <- TRUE # update \sigma and \nu
        blocks[[p + 4]][1:npar] <- TRUE # update all parameters
    }
      else if (blocking == "std6") {
        blocks <- replicate(2 * p + 4, rep(FALSE, npar), simplify = FALSE)
        for (i in 0:p) blocks[[i + 1]][c(i * nknots + 1:nknots, 
            nknots * (p + 1) + i + 1)] <- TRUE
        blocks[[p + 2]][nknots * (p + 1) + 1:(p + 1)] <- TRUE
        blocks[[p + 3]][(nknots + 1) * (p + 1) + 1:2] <- TRUE
        blocks[[p + 4]][1:npar] <- TRUE
        for (i in 1:p) blocks[[p + 4 + i]][c(1:nknots, i * nknots + 
            1:nknots)] <- TRUE # always update w_0 when updating w_1 to w_p
      }
      else {
        blocks <- replicate(npar, rep(FALSE, npar), simplify = FALSE)
        for (i in 1:npar) blocks[[i]][i] <- TRUE
      }
  
      
      # number of blocks
      nblocks <- length(blocks)
      if (fix.nu) 
          for (j in 1:nblocks) blocks[[j]][(nknots + 1) * (p + 
              1) + 2] <- FALSE # don't update nu
      blocks.ix <- c(unlist(lapply(blocks, which))) - 1
      blocks.size <- sapply(blocks, sum) # #params to be update in each block
      if (missing(blocks.mu)) 
          blocks.mu <- rep(0, sum(blocks.size))
      if (missing(blocks.S)) {
          sig.nu <- c(TRUE, !as.logical(fix.nu))
          blocks.S <- lapply(blocks.size, function(q) diag(1, q))
          if (substr(blocking, 1, 3) == "std") { # "std0" to "std7"
              for (i in 1:(p + 1)) blocks.S[[i]][1:nknots, 1:nknots] <- K0 # first p+1 blocks
              if (as.numeric(substr(blocking, 4, 5)) > 1) { # “std2” to "std7"
                  if (!illcond) 
                    blocks.S[[p + 2]] <- summary(suppressWarnings(quantreg::rq(quantreg::dither(y) ~ 
                      x, tau = 0.5, weights = wt)), se = "boot", 
                      cov = TRUE)$cov
                  blocks.S[[p + 3]] <- matrix(c(1, 0, 0, 0.1), 
                    2, 2)[sig.nu, sig.nu]
              }
              if (as.numeric(substr(blocking, 4, 5)) > 4) { # “std5-std7”
                  slist <- list()
                  length(slist) <- p + 3
                  for (i in 1:(p + 1)) slist[[i]] <- K0
                  if (illcond) {
                    slist[[p + 2]] <- diag(1, p + 1)
                  }
                  else {
                    slist[[p + 2]] <- summary(suppressWarnings(quantreg::rq(dither(y) ~ 
                      x, tau = 0.5, weights = wt)), se = "boot", 
                      cov = TRUE)$cov
                  }
                  slist[[p + 3]] <- matrix(c(1, 0, 0, 0.1), 2, 
                    2)[sig.nu, sig.nu]
                  blocks.S[[p + 4]] <- as.matrix(bdiag(slist)) # all parameters
                  if (blocking == "std6") {
                    for (i in 1:p) blocks.S[[p + 4 + i]] <- as.matrix(bdiag(slist[c(1, 
                      i + 1)]))
                  }
              }
          }
          blocks.S <- unlist(blocks.S)
      }
  }
    
    
    
    imcmc.par <- c(nblocks, ref.size, verbose, max(10, niter/10000), # ticker = max(10,niter/10000), print results every niter/ticker iterations
        rep(0, nblocks))
    dmcmc.par <- c(temp, 0.999, rep(acpt.target, nblocks), 2.38/sqrt(blocks.size))
    set.seed(10)
    tm.c <- system.time(oo <- .C("BJQRL", par = as.double(par),
        x = as.double(x), y = as.double(y), cens = as.integer(cens), 
        wt = as.double(wt), shrink = as.integer(shrink), hyper = as.double(hyper.reduced), 
        dim = as.integer(dimpars), gridmats = as.double(gridmats), 
        tau.g = as.double(tau.g), muV = as.double(blocks.mu), 
        SV = as.double(blocks.S), blocks = as.integer(blocks.ix), 
        blocks.size = as.integer(blocks.size), dmcmcpar = as.double(dmcmc.par), 
        imcmcpar = as.integer(imcmc.par), parsamp = double(nsamp * 
            length(par)), acptsamp = double(nsamp * nblocks), 
        lpsamp = double(nsamp), fbase.choice = as.integer(fbase.choice), sparse = as.integer(sparse)))
    
    if (verbose) 
        cat("elapsed time:", round(tm.c[3]), "seconds\n")
    doo$x <- x
    oo$y <- y
    oo$xnames <- colnames(x)
    oo$terms <- mt
    oo$gridmats <- gridmats
    oo$prox <- prox.grid
    oo$reg.ix <- reg.ix
    oo$runtime <- tm.c[3]
    class(oo) <- "qrjoint"
    return(oo)
#}
```

```{r}
set.seed(8848)
oo.sparse <- qrjointL(X9 ~ ., sim_data, nsamp = 100, thin = 20)
oo.nonsparse <- qrjoint(V8 ~ ., as.data.frame(data[,,1]), nsamp = 100, thin = 70)
```
```{r}
summary(models.ori.1000[[6]], more.details = T)
summaryL(models.1000[[54]],more.details = T)
```


```{r}
results = as_tibble(matrix(oo$parsamp,500,65,TRUE))
results = matrix(oo$parsamp,500,65,TRUE)
summary(oo,more.details = TRUE)

results2 = tibble(matrix(model$parsamp,200,73,TRUE))
results2 = matrix(model$parsamp,200,73,TRUE)
summary(fit.qrj,more.details = TRUE)
```

